{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea7ceb8",
   "metadata": {},
   "source": [
    "# What is the sentiment around COVID tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0894ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import seaborn as sns\n",
    "import string\n",
    "\n",
    "from io import StringIO\n",
    "from nltk.corpus import stopwords \n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split  \n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b291d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OriginalTweet           Sentiment\n",
       "0     TRENDING: New Yorkers encounter empty supermar...  Extremely Negative\n",
       "1     When I couldn't find hand sanitizer at Fred Me...            Positive\n",
       "2     Find out how you can protect yourself and love...  Extremely Positive\n",
       "3     #Panic buying hits #NewYork City as anxious sh...            Negative\n",
       "4     #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral\n",
       "...                                                 ...                 ...\n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive\n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative\n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral\n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative\n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive\n",
       "\n",
       "[3798 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Data\n",
    "covidDat = pd.read_csv(\"Corona_NLP_test.csv\")[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "covidDat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ecd60",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b82ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stopwords\n",
    "sw = set(stopwords.words(\"english\")) \n",
    "\n",
    "# a function that cleans text and removes stop words\n",
    "def clean(text, stopwords):\n",
    "    # remove tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text) # split text on whitespace\n",
    "    text_list = text.split()\n",
    "    text_words = []\n",
    "    \n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.add(\"”\")\n",
    "    punctuation.add(\"“\")\n",
    "    punctuation.add(\"‘\")\n",
    "    punctuation.add(\"’\")\n",
    "    punctuation.add(\",”\")\n",
    "    \n",
    "    # keep #tags and @mentions\n",
    "    ## punctuation.remove(\"#\")\n",
    "    ## punctuation.remove(\"@\")\n",
    "    \n",
    "    for word in text_list:\n",
    "        # remove punctuation marks at the beginning of each word\n",
    "        while len(word) > 0 and word[0] in punctuation:\n",
    "            word = word[1:]\n",
    "            \n",
    "        # remove punctuation marks at the end of each word\n",
    "        while len(word) > 0 and word[-1] in punctuation: \n",
    "            word = word[:-1]\n",
    "            \n",
    "        # a rule to eliminate most urls\n",
    "        if len(word) > 0 and \"/\" not in word: \n",
    "            # eliminate stopwords\n",
    "            if word.lower() not in stopwords:\n",
    "                # append the word to the text_words list\n",
    "                text_words.append(word.lower()) \n",
    "        cleaner_text = \" \".join(text_words)\n",
    "    return cleaner_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff7f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-out online grocers (FoodKick, MaxDelivery) as #coronavirus-fearing shoppers stock up https://t.co/Gr76pcrLWh https://t.co/ivMKMsqdT1\n"
     ]
    }
   ],
   "source": [
    "# Clean the Data\n",
    "\n",
    "# condense the Sentiment column\n",
    "def change_sen(sentiment):\n",
    "    if sentiment == \"Extremely Positive\":\n",
    "        return 'positive'\n",
    "    elif sentiment == \"Extremely Negative\":\n",
    "        return 'negative'\n",
    "    elif sentiment == \"Positive\":\n",
    "        return 'positive'\n",
    "    elif sentiment == \"Negative\":\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'netural'\n",
    "    \n",
    "covidDat[\"Sentiment\"] = covidDat[\"Sentiment\"].apply(lambda x:change_sen(x))\n",
    "\n",
    "# encode the Sentiment column\n",
    "#le = LabelEncoder()\n",
    "#covidDat['Sentiment'] = covidDat['Sentiment'].apply(lambda x:change_sen(x))\n",
    "\n",
    "print(covidDat[\"OriginalTweet\"].iloc[0])\n",
    "\n",
    "covidDat[\"OriginalTweet\"] = covidDat[\"OriginalTweet\"].apply(lambda x: clean(x, stopwords=sw))\n",
    "                                                           \n",
    "# select only text with more than 60 words for training\n",
    "covidDat = covidDat[covidDat[\"OriginalTweet\"].str.len() > 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b347c92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trending new yorkers encounter empty supermarket shelves pictured wegmans brooklyn sold-out online grocers foodkick maxdelivery coronavirus-fearing shoppers stock'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidDat[\"OriginalTweet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b036c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3581, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidDat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0603a1f",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b04445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2506,)\n",
      "(1075,)\n",
      "(2506,)\n",
      "(1075,)\n"
     ]
    }
   ],
   "source": [
    "# Split the Data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(covidDat[\"OriginalTweet\"], \n",
    "                                                    covidDat[\"Sentiment\"], \n",
    "                                                    test_size = .3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33e888",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561a0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Data\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), \n",
    "                        stop_words=\"english\", \n",
    "                        min_df=10)\n",
    "X_train_tfidf= tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3e7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  ['netural' 'netural' 'netural' ... 'netural' 'netural' 'netural']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Model\n",
    "\n",
    "#construct and fit a model\n",
    "mnb = MultinomialNB()\n",
    "mnb = mnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# make a prediction on training set\n",
    "print(\"Prediction: \", mnb.predict(X_train_tfidf))\n",
    "\n",
    "# compute accuracy on training set\n",
    "mnb.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa16e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "mnb.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ce03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score:  1.0\n",
      "Standard deviation of cross validation scores:  0.0\n",
      "The best parameters include:  {'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 1)}\n",
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Tune the Model\n",
    "\n",
    "#Cross Validation\n",
    "scores = cross_val_score(estimator = MultinomialNB(), \n",
    "                         X = X_train_tfidf, y = y_train, cv=5) \n",
    "print(\"Average cross validation score: \", scores.mean())\n",
    "print(\"Standard deviation of cross validation scores: \", scores.std())\n",
    "\n",
    "#Grid Search Cross Validation\n",
    "pipe = Pipeline([(\"tfidf\",TfidfVectorizer(stop_words=\"english\")), \n",
    "                 (\"mnb\", MultinomialNB())])\n",
    "param_grid = [{\"tfidf__min_df\":[5, 20],\n",
    "               \"tfidf__ngram_range\":[(1, 1), (1, 2), (1, 3)]}]\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe , param_grid =param_grid, cv=5) \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters include: \", grid.best_params_)\n",
    "print(\"Training Accuracy: \", grid.score(X_train, y_train))\n",
    "print(\"Testing Accuracy: \", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358077b",
   "metadata": {},
   "source": [
    "# Conclusion and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ac2c5",
   "metadata": {},
   "source": [
    "## Mini-Project Report\n",
    "\n",
    "### Problem to address:  \n",
    "##### To explore the sentiment of tweets about COVID\n",
    "\n",
    "### Description of the data:\n",
    "##### 3798 tweets which originally contained 6 columns but was reduced to just two columns--the original tweet and the sentiment.  By eliminating tweets with less than 60 words, the number of tweets was reduced to 3581.  This set of data was downloaded from Kaggle and the information included the fact that the originator hand-coded the dependent variable.\n",
    "\n",
    "### The choice of algorithm: \n",
    "##### The Multinomial Naive Bayes algorithm was chosen because it is used for text classification when data is represented as feature vectors\n",
    "\n",
    "### The performance of the algorithm: \n",
    "##### The algorithm had a 71% accuracy rate on the training set.  It had a 61% accuracy on the test data.\n",
    "\n",
    "### Overfitting? \n",
    "##### Because the model is not that good, there is probabily underfitting.  The overall prediction is pretty low.  However if the accuracy is high enough, then there is some overfitting because the accuracy of model on the training set is better than the accuracy on the test data.\n",
    "\n",
    "### Choice of hyperparameters tunned: \n",
    "##### \"tfidf__min_df\":[5, 20] and \"tfidf__ngram_range\":[(1, 1), (1, 2), (1, 3) were chosen and the best parameters were: 'tfidf__min_df': 5 and 'tfidf__ngram_range': (1, 1) .\n",
    "\n",
    "### Recommendation/Conclusion: \n",
    "##### Because of the text based nature of this data the Multinomial Naive Bayes is a good choice for the model.  Better accuracy might be achieved with different parameters.  Also, it should be noted that the author of this data included that he hand scored the prediction variable.  They could be the reason for the lower accuracy--human error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500bec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
